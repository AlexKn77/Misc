{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Julia: Logistic Regression\n",
    "\n",
    "## Part I: *Logistic regression without regularization*\n",
    "\n",
    "Predicting if a student will be accepted into a university based off of two test scores\n",
    "\n",
    "Beginning with package imports, data loading, and initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Exam1Score</th><th>Exam2Score</th><th>Admitted</th><th>x0</th></tr></thead><tbody><tr><th>1</th><td>34.62365962451697</td><td>78.0246928153624</td><td>0</td><td>1.0</td></tr><tr><th>2</th><td>30.28671076822607</td><td>43.89499752400101</td><td>0</td><td>1.0</td></tr><tr><th>3</th><td>35.84740876993872</td><td>72.90219802708364</td><td>0</td><td>1.0</td></tr><tr><th>4</th><td>60.18259938620976</td><td>86.30855209546826</td><td>1</td><td>1.0</td></tr><tr><th>5</th><td>79.0327360507101</td><td>75.3443764369103</td><td>1</td><td>1.0</td></tr><tr><th>6</th><td>45.08327747668339</td><td>56.3163717815305</td><td>0</td><td>1.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×4 DataFrames.DataFrame\n",
       "│ Row │ Exam1Score │ Exam2Score │ Admitted │ x0  │\n",
       "├─────┼────────────┼────────────┼──────────┼─────┤\n",
       "│ 1   │ 34.6237    │ 78.0247    │ 0        │ 1.0 │\n",
       "│ 2   │ 30.2867    │ 43.895     │ 0        │ 1.0 │\n",
       "│ 3   │ 35.8474    │ 72.9022    │ 0        │ 1.0 │\n",
       "│ 4   │ 60.1826    │ 86.3086    │ 1        │ 1.0 │\n",
       "│ 5   │ 79.0327    │ 75.3444    │ 1        │ 1.0 │\n",
       "│ 6   │ 45.0833    │ 56.3164    │ 0        │ 1.0 │"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "using Plots\n",
    "using GLM  # For comparing answers\n",
    "using Optim  # For optimizing thetas\n",
    "\n",
    "df = readtable(\"ex2/ex2data1.txt\", header=false)\n",
    "names!(df, [:Exam1Score, :Exam2Score, :Admitted])\n",
    "\n",
    "# Adding the intercept term\n",
    "df[:x0] = ones(nrow(df))\n",
    "\n",
    "X = df[[:x0, :Exam1Score, :Exam2Score]]\n",
    "y = df[:Admitted]\n",
    "\n",
    "# An array of 0s for starting values of theta to be used in many functions\n",
    "initialTheta = zeros(3)\n",
    "\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"C:\\Users\\JeffM\\.julia\\v0.5\\Plots\\src\\backends\\..\\..\\deps\\plotly-latest.min.js\"></script>    <div id=\"798e0fcf-9d0f-43c3-8471-3ed66da2c1d3\" style=\"width:600px;height:400px;\"></div>\n",
       "    <script>\n",
       "    PLOT = document.getElementById('798e0fcf-9d0f-43c3-8471-3ed66da2c1d3');\n",
       "    Plotly.plot(PLOT, [{\"yaxis\":\"y\",\"y\":[86.30855209546826,75.3443764369103,96.51142588489624,46.55401354116538,87.42056971926803,43.53339331072109,76.48196330235604,97.71869196188608,89.20735013750205,52.74046973016765,92.92713789364831,47.57596364975532,65.79936592745237,68.9723599933059,69.95445795447587,44.82162893218353,72.80788731317097,57.05198397627122,69.43286012045222,80.90806058670817,56.30804621605327,65.56892160559052,70.45820000180959,86.7278223300282,96.76882412413983,88.696292545466,74.16311935043758,60.99903099844988,43.39060180650027,68.86157272420604,69.82457122657193,78.45356224515052,85.75993667331619,97.53518548909936,92.11606081344084,60.99139402740988,78.03168802018232,96.22759296761404,73.09499809758037,75.85844831279042,72.36925193383885,88.47586499559782,75.80985952982456,69.80378889835472,45.69430680250754,66.58935317747915,59.51428198012956,90.96014789746954,85.59430710452014,90.42453899753964,96.64742716885644,77.15910509073893,87.50879176484702,84.84513684930135,45.35828361091658,48.38028579728175,87.10385094025457,68.77540947206617,64.9319380069486,89.52981289513276],\"showlegend\":true,\"name\":\"Admitted\",\"type\":\"scatter\",\"xaxis\":\"x\",\"x\":[60.18259938620976,79.0327360507101,61.10666453684766,75.02474556738889,76.09878670226257,84.43281996120035,82.30705337399482,69.36458875970939,53.9710521485623,69.07014406283025,70.66150955499435,76.97878372747498,89.6767757507208,77.9240914545704,62.27101367004632,80.1901807509566,61.379289447425,85.40451939411645,52.04540476831827,64.17698887494485,83.90239366249155,94.44336776917852,77.19303492601364,97.77159928000232,62.07306379667647,91.56497449807442,79.94481794066932,99.2725269292572,90.54671411399852,97.64563396007767,74.24869136721598,71.7964620586338,75.3956114656803,40.45755098375164,80.27957401466998,66.74671856944039,64.0393204150601,72.34649422579923,60.45788573918959,58.84095621726802,99.82785779692128,47.26426910848174,50.45815980285988,88.9138964166533,94.83450672430196,67.31925746917527,57.23870631569862,80.36675600171273,68.46852178591112,75.47770200533905,78.63542434898018,94.09433112516793,90.44855097096364,74.49269241843041,89.84580670720979,83.48916274498238,42.2617008099817,99.31500880510394,55.34001756003703,74.77589300092767],\"mode\":\"markers\",\"marker\":{\"symbol\":\"circle\",\"line\":{\"width\":1,\"color\":\"rgba(0, 0, 0, 1.000)\"},\"size\":8,\"color\":\"rgba(0, 154, 250, 1.000)\"}},{\"yaxis\":\"y\",\"y\":[78.0246928153624,43.89499752400101,72.90219802708364,56.3163717815305,38.22527805795094,30.60326323428011,76.03681085115882,46.67857410673128,42.83843832029179,48.85581152764205,44.20952859866288,38.80067033713209,50.25610789244621,64.99568095539578,63.12762376881715,71.16774802184875,52.21388588061123,98.86943574220611,41.57341522824434,75.2377203360134,46.85629026349976,40.61825515970618,45.82270145776001,52.06099194836679,60.39634245837173,49.80453881323059,59.80895099453265,95.59854761387875,47.02051394723416,39.26147251058019,49.59297386723685,66.45008614558913,41.09209807936973,51.88321182073966,43.30717306430063,42.50840943572217,42.71987853716458,78.84478600148043,60.76950525602592,35.57070347228866],\"showlegend\":true,\"name\":\"Not Admitted\",\"type\":\"scatter\",\"xaxis\":\"x\",\"x\":[34.62365962451697,30.28671076822607,35.84740876993872,45.08327747668339,95.86155507093572,75.01365838958247,39.53833914367223,67.94685547711617,67.37202754570876,50.534788289883,34.21206097786789,93.114388797442,61.83020602312595,38.78580379679423,52.10797973193984,40.23689373545111,54.63510555424817,33.91550010906887,74.78925295941542,34.1836400264419,51.54772026906181,82.36875375713919,51.04775177128865,62.22267576120188,34.52451385320009,50.2864961189907,49.58667721632031,32.57720016809309,35.28611281526193,56.25381749711624,30.05882244669796,44.66826172480893,66.56089447242954,49.07256321908844,32.72283304060323,60.45555629271532,82.22666157785568,42.0754545384731,52.34800398794107,55.48216114069585],\"mode\":\"markers\",\"marker\":{\"symbol\":\"circle\",\"line\":{\"width\":1,\"color\":\"rgba(0, 0, 0, 1.000)\"},\"size\":8,\"color\":\"rgba(227, 111, 71, 1.000)\"}}], {\"yaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"Exam 2 Score\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[50.0,100.0],\"domain\":[0.07581474190726165,0.9901574803149606],\"ticktext\":[\"50\",\"100\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"x\"},\"annotations\":[],\"width\":600,\"plot_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"y\":1.0,\"font\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"bordercolor\":\"rgba(0, 0, 0, 1.000)\",\"x\":1.0},\"xaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"Exam 1 Score\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[50.0,100.0],\"domain\":[0.07646908719743364,0.9934383202099737],\"ticktext\":[\"50\",\"100\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"y\"},\"paper_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"height\":400,\"margin\":{\"r\":0,\"l\":0,\"b\":0,\"t\":20}});\n",
       "    </script>\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the data\n",
    "# Subsetting to plot each group separately in order to get separate colors\n",
    "admitted = df[(df[:Admitted].==1),:]\n",
    "notAdmitted = df[(df[:Admitted].==0),:]\n",
    "\n",
    "scatter(admitted[:Exam1Score], admitted[:Exam2Score],\n",
    "    label=\"Admitted\", xlab=\"Exam 1 Score\", ylab=\"Exam 2 Score\")\n",
    "scatter!(notAdmitted[:Exam1Score], notAdmitted[:Exam2Score],\n",
    "    label=\"Not Admitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Converts $z$ into a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition "
     ]
    },
    {
     "data": {
      "text/html": [
       "<script src=\"C:\\Users\\JeffM\\.julia\\v0.5\\Plots\\src\\backends\\..\\..\\deps\\plotly-latest.min.js\"></script>    <div id=\"7481097f-0f02-47b0-bbb7-474dec0e09f2\" style=\"width:600px;height:400px;\"></div>\n",
       "    <script>\n",
       "    PLOT = document.getElementById('7481097f-0f02-47b0-bbb7-474dec0e09f2');\n",
       "    Plotly.plot(PLOT, [{\"yaxis\":\"y\",\"y\":[4.5397868702434395e-5,0.00012339457598623172,0.0003353501304664781,0.0009110511944006454,0.0024726231566347743,0.0066928509242848554,0.01798620996209156,0.04742587317756678,0.11920292202211755,0.2689414213699951,0.5,0.7310585786300049,0.8807970779778823,0.9525741268224334,0.9820137900379085,0.9933071490757153,0.9975273768433653,0.9990889488055994,0.9996646498695336,0.9998766054240137,0.9999546021312976],\"showlegend\":true,\"name\":\"y1\",\"type\":\"scatter\",\"xaxis\":\"x\",\"line\":{\"width\":1,\"dash\":\"solid\",\"color\":\"rgba(0, 154, 250, 1.000)\",\"shape\":\"linear\"},\"x\":[-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10],\"mode\":\"lines\"}], {\"yaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[0.25,0.5,0.75],\"domain\":[0.03762029746281716,0.9901574803149606],\"ticktext\":[\"0.25\",\"0.50\",\"0.75\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"x\"},\"annotations\":[],\"width\":600,\"plot_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"y\":1.0,\"font\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"bordercolor\":\"rgba(0, 0, 0, 1.000)\",\"x\":1.0},\"xaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[-10.0,-5.0,0.0,5.0,10.0],\"domain\":[0.0658209390492855,0.9934383202099738],\"ticktext\":[\"-10\",\"-5\",\"0\",\"5\",\"10\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"y\"},\"paper_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"height\":400,\"margin\":{\"r\":0,\"l\":0,\"b\":0,\"t\":20}});\n",
       "    </script>\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sigmoid(Any) in module Main at In[69]:3 overwritten at In[113]:3.\n"
     ]
    }
   ],
   "source": [
    "function sigmoid(z)\n",
    "    # Converts numerical input into a value between 0 and 1\n",
    "    z = 1/(1+exp(-z))\n",
    "    return z\n",
    "end\n",
    "\n",
    "# Plotting values to validate the function is working correctly\n",
    "plot(collect(-10:10), \n",
    "     sigmoid.(collect(-10:10)))  # f. applies the function to the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Hypothesis\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx)$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $g$: Sigmoid function\n",
    "\n",
    "    - $\\theta^T$: Transposed parameters\n",
    "       \n",
    "        - E.x.: $\\theta^T = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logisticHypothesis"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Any, Any) in module Main at In[68]:4 overwritten at In[114]:4.\n"
     ]
    }
   ],
   "source": [
    "function logisticHypothesis(theta, X)\n",
    "    # Calculates the hypothesis for X given values of\n",
    "    # theta for logistic regression\n",
    "    X = convert(Array, X)\n",
    "    h = sigmoid.(*(X, theta))\n",
    "    return h\n",
    "end\n",
    "\n",
    "logisticHypothesis(initialTheta, X)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $m$: Number of records\n",
    "\n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition costFunction(Any, Any"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6931471805599452"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", Any) in module Main at In[90]:3 overwritten at In[115]:3.\n"
     ]
    }
   ],
   "source": [
    "function costFunction(theta, X, y)\n",
    "    # Computes cost for logistic regression\n",
    "    X = convert(Array, X)\n",
    "    y = convert(Array, y)\n",
    "    m = length(y)\n",
    "    \n",
    "    h = logisticHypothesis(theta, X)\n",
    "    error = sum(-y.*log(h)-(1-y).*log(1-h))\n",
    "    J = (1/m)*error\n",
    "    return J\n",
    "end\n",
    "\n",
    "costFunction(initialTheta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\partial$: Partial derivative\n",
    "    \n",
    "    - $J(\\theta)$: Cost given $\\theta$\n",
    "\n",
    "    - $m$: Number of records\n",
    "    \n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)\n",
    "    \n",
    "We won't actually be using this function to find the optimal values of $\\theta_j$, so this is just illustrating the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logisticGradient"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  -0.1   \n",
       " -12.0092\n",
       " -11.2628"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Any, Any, Any) in module Main at In[101]:3 overwritten at In[116]:3.\n"
     ]
    }
   ],
   "source": [
    "function logisticGradient(theta, X, y)\n",
    "    # Computes the gradient for logistic regression\n",
    "    X = convert(Array, X)\n",
    "    y = convert(Array, y)\n",
    "    m = length(y)\n",
    "    \n",
    "    h = logisticHypothesis(theta, X)\n",
    "    gradient = (1/m) * (*(transpose(X), (h-y)))\n",
    "    return gradient\n",
    "end\n",
    "\n",
    "logisticGradient(initialTheta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta_j$ for the cost function using the base R optim function.  This is similar to MATLAB's fminunc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching costFunction(::DataFrames.DataFrame, ::DataArrays.DataArray{Int64,1})\u001b[0m\nClosest candidates are:\n  costFunction(::Any, ::Any, \u001b[1m\u001b[31m::Any\u001b[0m) at In[115]:3\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching costFunction(::DataFrames.DataFrame, ::DataArrays.DataArray{Int64,1})\u001b[0m\nClosest candidates are:\n  costFunction(::Any, ::Any, \u001b[1m\u001b[31m::Any\u001b[0m) at In[115]:3\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "optimCostFunc = costFunction(initialTheta, X, y)\n",
    "\n",
    "optimize(costFunction(X, y), \n",
    "# optimize(optimCostFunc, \n",
    "         logisticGradient(X, y), \n",
    "         initialTheta, \n",
    "#          X, y, \n",
    "         BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the obtained parameters to what Julia's **[insert function here]** function provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on if they are similar]**\n",
    "\n",
    "Calculating the class probability and generating predictions of acceptance using values of $\\theta_j$ obtained from the optimization function\n",
    "\n",
    "The outputs from logistic regression are just the class probability, or $P(y = 1 \\mid x; \\theta)$, so we are predicting the classes (accepted or not) as follows:\n",
    "\n",
    "$Prediction(y \\mid x; \\theta) = \\begin{cases} 1, \\quad\\mbox{ if } P(y = 1 \\mid x; \\theta) > 0.50 \\\\ 0, \\quad\\mbox{ if } P(y = 1 \\mid x; \\theta) \\leq 0.50 \\end{cases} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision boundary over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Part II: *Logistic regression with regularization*\n",
    "\n",
    "Predicting if a microchip passes QA after two tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Test1</th><th>Test2</th><th>Accepted</th><th>x0</th></tr></thead><tbody><tr><th>1</th><td>0.051267</td><td>0.69956</td><td>1</td><td>1.0</td></tr><tr><th>2</th><td>-0.092742</td><td>0.68494</td><td>1</td><td>1.0</td></tr><tr><th>3</th><td>-0.21371</td><td>0.69225</td><td>1</td><td>1.0</td></tr><tr><th>4</th><td>-0.375</td><td>0.50219</td><td>1</td><td>1.0</td></tr><tr><th>5</th><td>-0.51325</td><td>0.46564</td><td>1</td><td>1.0</td></tr><tr><th>6</th><td>-0.52477</td><td>0.2098</td><td>1</td><td>1.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×4 DataFrames.DataFrame\n",
       "│ Row │ Test1     │ Test2   │ Accepted │ x0  │\n",
       "├─────┼───────────┼─────────┼──────────┼─────┤\n",
       "│ 1   │ 0.051267  │ 0.69956 │ 1        │ 1.0 │\n",
       "│ 2   │ -0.092742 │ 0.68494 │ 1        │ 1.0 │\n",
       "│ 3   │ -0.21371  │ 0.69225 │ 1        │ 1.0 │\n",
       "│ 4   │ -0.375    │ 0.50219 │ 1        │ 1.0 │\n",
       "│ 5   │ -0.51325  │ 0.46564 │ 1        │ 1.0 │\n",
       "│ 6   │ -0.52477  │ 0.2098  │ 1        │ 1.0 │"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = readtable(\"ex2/ex2data2.txt\", header=false)\n",
    "names!(df, [:Test1, :Test2, :Accepted])\n",
    "\n",
    "# Adding the intercept term\n",
    "df[:x0] = ones(nrow(df))\n",
    "\n",
    "X = df[[:x0, :Test1, :Test2]]\n",
    "y = df[:Accepted]\n",
    "\n",
    "# An array of 0s for starting values of theta to be used in many functions\n",
    "initialTheta = zeros(3)\n",
    "\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"C:\\Users\\JeffM\\.julia\\v0.5\\Plots\\src\\backends\\..\\..\\deps\\plotly-latest.min.js\"></script>    <div id=\"e58ad7f9-97d6-4383-9ffb-7c8350607f01\" style=\"width:600px;height:400px;\"></div>\n",
       "    <script>\n",
       "    PLOT = document.getElementById('e58ad7f9-97d6-4383-9ffb-7c8350607f01');\n",
       "    Plotly.plot(PLOT, [{\"yaxis\":\"y\",\"y\":[0.69956,0.68494,0.69225,0.50219,0.46564,0.2098,0.034357,-0.19225,-0.40424,-0.51389,-0.56506,-0.5212,-0.24342,-0.18494,0.48757,0.5826,0.53874,0.81652,0.69956,0.63377,0.59722,0.33406,0.005117,-0.27266,-0.39693,-0.60161,-0.53582,-0.53582,0.54605,0.77997,0.96272,0.8019,0.64839,0.47295,0.31213,0.027047,-0.21418,-0.18494,-0.16301,-0.41155,-0.2288,-0.18494,-0.14108,0.012427,0.15863,0.26827,0.44371,0.52412,0.67032,0.69225,0.57529,0.39985,0.55336,0.35599,0.17325,0.21711,-0.016813,-0.27266],\"showlegend\":true,\"name\":\"Accepted\",\"type\":\"scatter\",\"xaxis\":\"x\",\"x\":[0.051267,-0.092742,-0.21371,-0.375,-0.51325,-0.52477,-0.39804,-0.30588,0.016705,0.13191,0.38537,0.52938,0.63882,0.73675,0.54666,0.322,0.16647,-0.046659,-0.17339,-0.47869,-0.60541,-0.62846,-0.59389,-0.42108,-0.11578,0.20104,0.46601,0.67339,-0.13882,-0.29435,-0.26555,-0.16187,-0.17339,-0.28283,-0.36348,-0.30012,-0.23675,-0.06394,0.062788,0.22984,0.2932,0.48329,0.64459,0.46025,0.6273,0.57546,0.72523,0.22408,0.44297,0.322,0.13767,-0.0063364,-0.092742,-0.20795,-0.20795,-0.43836,-0.21947,-0.13882],\"mode\":\"markers\",\"marker\":{\"symbol\":\"circle\",\"line\":{\"width\":1,\"color\":\"rgba(0, 0, 0, 1.000)\"},\"size\":8,\"color\":\"rgba(0, 154, 250, 1.000)\"}},{\"yaxis\":\"y\",\"y\":[0.93348,0.77997,0.61915,0.75804,0.7288,0.59722,0.50219,0.3633,0.27558,0.085526,0.012427,-0.082602,-0.20687,-0.36769,-0.5212,-0.55775,-0.7405,-0.5943,-0.41886,-0.57968,-0.76974,-0.75512,-0.57968,-0.4481,-0.41155,-0.25804,-0.25804,0.041667,0.2902,0.68494,0.70687,0.91886,0.90424,0.70687,0.77997,0.91886,0.99196,1.1089,1.087,0.82383,0.88962,0.66301,0.64108,0.10015,-0.57968,-0.63816,-0.36769,-0.3019,-0.13377,-0.060673,-0.067982,-0.21418,-0.41886,-0.082602,0.31213,0.53874,0.49488,0.99927,0.99927,-0.030612],\"showlegend\":true,\"name\":\"Not Accepted\",\"type\":\"scatter\",\"xaxis\":\"x\",\"x\":[0.18376,0.22408,0.29896,0.50634,0.61578,0.60426,0.76555,0.92684,0.82316,0.96141,0.93836,0.86348,0.89804,0.85196,0.82892,0.79435,0.59274,0.51786,0.46601,0.35081,0.28744,0.085829,0.14919,-0.13306,-0.40956,-0.39228,-0.74366,-0.69758,-0.75518,-0.69758,-0.4038,-0.38076,-0.50749,-0.54781,0.10311,0.057028,-0.10426,-0.081221,0.28744,0.39689,0.63882,0.82316,0.67339,1.0709,-0.046659,-0.23675,-0.15035,-0.49021,-0.46717,-0.28859,-0.61118,-0.66302,-0.59965,-0.72638,-0.83007,-0.72062,-0.59389,-0.48445,-0.0063364,0.63265],\"mode\":\"markers\",\"marker\":{\"symbol\":\"circle\",\"line\":{\"width\":1,\"color\":\"rgba(0, 0, 0, 1.000)\"},\"size\":8,\"color\":\"rgba(227, 111, 71, 1.000)\"}}], {\"yaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"Microchip Test 2\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[-0.5,0.0,0.5,1.0],\"domain\":[0.07581474190726165,0.9901574803149606],\"ticktext\":[\"-0.5\",\"0.0\",\"0.5\",\"1.0\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"x\"},\"annotations\":[],\"width\":600,\"plot_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"y\":1.0,\"font\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"bordercolor\":\"rgba(0, 0, 0, 1.000)\",\"x\":1.0},\"xaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"Microchip Test 1\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[-0.5,0.0,0.5,1.0],\"domain\":[0.09128390201224845,0.9934383202099738],\"ticktext\":[\"-0.5\",\"0.0\",\"0.5\",\"1.0\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"y\"},\"paper_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"height\":400,\"margin\":{\"r\":0,\"l\":0,\"b\":0,\"t\":20}});\n",
       "    </script>\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the data\n",
    "# Subsetting to plot each group separately in order to get separate colors\n",
    "admitted = df[(df[:Accepted].==1),:]\n",
    "notAdmitted = df[(df[:Accepted].==0),:]\n",
    "\n",
    "scatter(admitted[:Test1], admitted[:Test2],\n",
    "    label=\"Accepted\", xlab=\"Microchip Test 1\", ylab=\"Microchip Test 2\")\n",
    "scatter!(notAdmitted[:Test1], notAdmitted[:Test2],\n",
    "    label=\"Not Accepted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping\n",
    "\n",
    "Maps the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.  This allows for a more complex and nonlinear decision boundary.  \n",
    "\n",
    "The feature space prior to feature mapping (3-dimensional vector): \n",
    "\n",
    "$\\hspace{1cm} Feature(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$ \n",
    "\n",
    "The feature space after feature mapping:\n",
    "\n",
    "$\\hspace{1cm} mapFeature(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_1x_2 \\\\ x_2^2 \\\\ x_1^3 \\\\ \\vdots \\\\ x_1x_2^5 \\\\ x_2^6 \\end{bmatrix}$\n",
    "\n",
    "**Note:** I made a few adjustments on the Octave/MATLAB code provided for this assignment in order to maintain the names of the polynomials\n",
    "Octave/MATLAB code:\n",
    "```\n",
    "degree = 6;\n",
    "out = ones(size(X1(:,1)));\n",
    "for i = 1:degree\n",
    "    for j = 0:i\n",
    "        out(:, end+1) = (X1.^(i-j)).*(X2.^j);\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234567"
     ]
    }
   ],
   "source": [
    "degree = 6\n",
    "for j in collect(1:degree+1)\n",
    "    print(j)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['Test1', 'Test2']].copy()\n",
    "\n",
    "y = df['Accepted'].copy()\n",
    "\n",
    "# Creating function for use in plotting decision boundaries later\n",
    "def map_features(X, degree=1):\n",
    "    \"\"\"\n",
    "    Maps the powers for X up to the degree specified, and appends them to X\n",
    "    Includes interaction terms and the intercept\n",
    "    \"\"\"\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range(i+1):\n",
    "            \n",
    "            # Assigning names to the columns\n",
    "            if j == 0:\n",
    "                if i == 1:\n",
    "                    colName = 'x1'\n",
    "                else:\n",
    "                    colName = 'x1_'+str(i)\n",
    "            elif (i-j) == 0:\n",
    "                if j == 1:\n",
    "                    colName = 'x2'\n",
    "                else:\n",
    "                    colName = 'x2_'+str(j)\n",
    "            else:\n",
    "                colName = 'x1_'+str(i-j)+':'+'x2_'+str(j)\n",
    "                \n",
    "            # Calculating polynomial features\n",
    "            X[colName] = np.power(X.iloc[:, 0],i-j) * np.power(X.iloc[:, 1],j)\n",
    "    \n",
    "    X = X.iloc[:, 2:]  # Removing original columns to keep naming conventions\n",
    "    X.insert(0, 'x0', 1)  # Inserting the intercept term\n",
    "    return X\n",
    "\n",
    "X = map_features(X, degree=6)\n",
    "\n",
    "# Creating a new list of initial thetas\n",
    "initialTheta = np.zeros(X.shape[1])\n",
    "            \n",
    "print('Dimensions:', X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "The only change from the other cost function we used earlier is the addition of the regularization parameter:\n",
    "\n",
    "#### Regularization Parameter\n",
    "\n",
    "$\\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\lambda$: The weight which the parameters are adjusted by.  A lower $\\lambda$ has little effect on the parameters, and a higher $\\lambda$ (e.x. $\\lambda = 1,000$) will adjust the parameters to be close to 0.\n",
    "    - $m$: Number of records\n",
    "    - $j$: The index for the parameter.  E.x. $\\theta_{j=1}$ is the score for Microchip Test #1\n",
    "\n",
    "**Note:** $\\theta_0$ should not be regularized as denoted by the summation in the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\begin{cases} \n",
    "\\hspace{0.25cm} \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} & \\text{for}\\ j = 0 \\\\\n",
    "\\Big(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\\Big) + \\frac{\\lambda}{m}\\theta_j & \\text{for}\\ j \\geq 1\n",
    "\\end{cases}$\n",
    "\n",
    "This is also the same as the last gradient with the exception of the regularization parameter\n",
    "\n",
    "#### Regularization Parameter\n",
    "\n",
    "$\\frac{\\lambda}{m}\\theta_j \\hspace{0.5cm}$for $j \\geq 1$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\lambda$: The weight which the parameters are adjusted by.  A lower $\\lambda$ has little effect on the parameters, and a higher $\\lambda$ (e.x. $\\lambda = 1,000$) will adjust the parameters to be close to 0.\n",
    "    - $m$: Number of records\n",
    "    - $j$: The index for the parameter.  E.x. $\\theta_{j=1}$ is the score for Microchip Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta$.  This chunk will take longer to run since we're dealing with a much higher dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking against Julia's **[Insert function here]** logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on the difference]**\n",
    "\n",
    "Lastly, comparing the accuracy between the two models.  Classification accuracy is just the percentage of records correctly classified (precision, recall, f-1 score, etc. offer more nuanced information on performance), so we will have to calculate the class probabilities and assign predictions like we did for part one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on the difference]**\n",
    "\n",
    "Plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
