{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Julia: Logistic Regression\n",
    "\n",
    "## *Part One*: Logistic regression without regularization\n",
    "\n",
    "Predicting if a student will be accepted into a university based off of two test scores\n",
    "\n",
    "Beginning with package imports, data loading, and initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Converts $z$ into a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Hypothesis\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx)$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $g$: Sigmoid function\n",
    "\n",
    "    - $\\theta^T$: Transposed parameters\n",
    "       \n",
    "        - E.x.: $\\theta^T = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $m$: Number of records\n",
    "\n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\partial$: Partial derivative\n",
    "    \n",
    "    - $J(\\theta)$: Cost given $\\theta$\n",
    "\n",
    "    - $m$: Number of records\n",
    "    \n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)\n",
    "    \n",
    "We won't actually be using this function to find the optimal values of $\\theta_j$, so this is just illustrating the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta_j$ for the cost function using the base R optim function.  This is similar to MATLAB's fminunc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the obtained parameters to what Julia's **[insert function here]** function provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on if they are similar]**\n",
    "\n",
    "Calculating the class probability and generating predictions of acceptance using values of $\\theta_j$ obtained from the optimization function\n",
    "\n",
    "The outputs from logistic regression are just the class probability, or $P(y = 1 \\mid x; \\theta)$, so we are predicting the classes (accepted or not) as follows:\n",
    "\n",
    "$Prediction(y \\mid x; \\theta) = \\begin{cases} 1, \\quad\\mbox{ if } P(y = 1 \\mid x; \\theta) > 0.50 \\\\ 0, \\quad\\mbox{ if } P(y = 1 \\mid x; \\theta) \\leq 0.50 \\end{cases} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision boundary over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  **Part 2:** Logistic regression with regularization\n",
    "\n",
    "Predicting if a microchip passes QA after two tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping\n",
    "\n",
    "Maps the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.  This allows for a more complex and nonlinear decision boundary.  \n",
    "\n",
    "The feature space prior to feature mapping (3-dimensional vector): \n",
    "\n",
    "$\\hspace{1cm} Feature(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$ \n",
    "\n",
    "The feature space after feature mapping:\n",
    "\n",
    "$\\hspace{1cm} mapFeature(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_1x_2 \\\\ x_2^2 \\\\ x_1^3 \\\\ \\vdots \\\\ x_1x_2^5 \\\\ x_2^6 \\end{bmatrix}$\n",
    "\n",
    "**Note:** The Octave/MATLAB code provided for this assignment resulted in a 28-dimensional feature space, but our code will result in a 49-dimensional feature space.  This is because the Octave/MATLAB code provided *some* interaction terms (e.x. $x_1x_2,\\ x_1^3x_2^4$, etc.), but not all possible interaction terms.  For simplicity, we'll use all interaction terms.\n",
    "\n",
    "Octave/MATLAB code:\n",
    "```\n",
    "degree = 6;\n",
    "out = ones(size(X1(:,1)));\n",
    "for i = 1:degree\n",
    "    for j = 0:i\n",
    "        out(:, end+1) = (X1.^(i-j)).*(X2.^j);\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "Corresponding Julia code:\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "The only change from the other cost function we used earlier is the addition of the regularization parameter:\n",
    "\n",
    "#### Regularization Parameter\n",
    "\n",
    "$\\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\lambda$: The weight which the parameters are adjusted by.  A lower $\\lambda$ has little effect on the parameters, and a higher $\\lambda$ (e.x. $\\lambda = 1,000$) will adjust the parameters to be close to 0.\n",
    "    - $m$: Number of records\n",
    "    - $j$: The index for the parameter.  E.x. $\\theta_{j=1}$ is the score for Microchip Test #1\n",
    "\n",
    "**Note:** $\\theta_0$ should not be regularized as denoted by the summation in the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\Big(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\\Big) + \\frac{\\lambda}{m}\\theta_j \\hspace{0.5cm}$for $j \\geq 1$\n",
    "\n",
    "This is also the same as the last gradient with the exception of the regularization parameter\n",
    "\n",
    "#### Regularization Parameter\n",
    "\n",
    "$\\frac{\\lambda}{m}\\theta_j \\hspace{0.5cm}$for $j \\geq 1$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\lambda$: The weight which the parameters are adjusted by.  A lower $\\lambda$ has little effect on the parameters, and a higher $\\lambda$ (e.x. $\\lambda = 1,000$) will adjust the parameters to be close to 0.\n",
    "    - $m$: Number of records\n",
    "    - $j$: The index for the parameter.  E.x. $\\theta_{j=1}$ is the score for Microchip Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta$.  This chunk will take longer to run since we're dealing with a much higher dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking against Julia's **[Insert function here]** logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on the difference]**\n",
    "\n",
    "Lastly, comparing the accuracy between the two models.  Classification accuracy is just the percentage of records correctly classified (precision, recall, f-1 score, etc. offer more nuanced information on performance), so we will have to calculate the class probabilities and assign predictions like we did for part one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on the difference]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
