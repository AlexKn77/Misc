{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Julia: Logistic Regression\n",
    "\n",
    "## *Part One*: Logistic regression without regularization\n",
    "\n",
    "Predicting if a student will be accepted into a university based off of two test scores\n",
    "\n",
    "Beginning with package imports, data loading, and initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Exam1Score</th><th>Exam2Score</th><th>Admitted</th><th>x0</th></tr></thead><tbody><tr><th>1</th><td>34.62365962451697</td><td>78.0246928153624</td><td>0</td><td>1.0</td></tr><tr><th>2</th><td>30.28671076822607</td><td>43.89499752400101</td><td>0</td><td>1.0</td></tr><tr><th>3</th><td>35.84740876993872</td><td>72.90219802708364</td><td>0</td><td>1.0</td></tr><tr><th>4</th><td>60.18259938620976</td><td>86.30855209546826</td><td>1</td><td>1.0</td></tr><tr><th>5</th><td>79.0327360507101</td><td>75.3443764369103</td><td>1</td><td>1.0</td></tr><tr><th>6</th><td>45.08327747668339</td><td>56.3163717815305</td><td>0</td><td>1.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×4 DataFrames.DataFrame\n",
       "│ Row │ Exam1Score │ Exam2Score │ Admitted │ x0  │\n",
       "├─────┼────────────┼────────────┼──────────┼─────┤\n",
       "│ 1   │ 34.6237    │ 78.0247    │ 0        │ 1.0 │\n",
       "│ 2   │ 30.2867    │ 43.895     │ 0        │ 1.0 │\n",
       "│ 3   │ 35.8474    │ 72.9022    │ 0        │ 1.0 │\n",
       "│ 4   │ 60.1826    │ 86.3086    │ 1        │ 1.0 │\n",
       "│ 5   │ 79.0327    │ 75.3444    │ 1        │ 1.0 │\n",
       "│ 6   │ 45.0833    │ 56.3164    │ 0        │ 1.0 │"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "using Plots\n",
    "using GLM  # For comparing answers\n",
    "\n",
    "df = readtable(\"ex2/ex2data1.txt\", header=false)\n",
    "names!(df, [:Exam1Score, :Exam2Score, :Admitted])\n",
    "\n",
    "# Adding the intercept term\n",
    "df[:x0] = ones(nrow(df))\n",
    "\n",
    "X = df[[:x0, :Exam1Score, :Exam2Score]]\n",
    "y = df[:Admitted]\n",
    "\n",
    "# An array of 0s for starting values of theta to be used in many functions\n",
    "initialTheta = zeros(3)\n",
    "\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"C:\\Users\\JeffM\\.julia\\v0.5\\Plots\\src\\backends\\..\\..\\deps\\plotly-latest.min.js\"></script>    <div id=\"cda3e849-cc72-40b6-b367-be95ac20a2e0\" style=\"width:600px;height:400px;\"></div>\n",
       "    <script>\n",
       "    PLOT = document.getElementById('cda3e849-cc72-40b6-b367-be95ac20a2e0');\n",
       "    Plotly.plot(PLOT, [{\"yaxis\":\"y\",\"y\":[86.30855209546826,75.3443764369103,96.51142588489624,46.55401354116538,87.42056971926803,43.53339331072109,76.48196330235604,97.71869196188608,89.20735013750205,52.74046973016765,92.92713789364831,47.57596364975532,65.79936592745237,68.9723599933059,69.95445795447587,44.82162893218353,72.80788731317097,57.05198397627122,69.43286012045222,80.90806058670817,56.30804621605327,65.56892160559052,70.45820000180959,86.7278223300282,96.76882412413983,88.696292545466,74.16311935043758,60.99903099844988,43.39060180650027,68.86157272420604,69.82457122657193,78.45356224515052,85.75993667331619,97.53518548909936,92.11606081344084,60.99139402740988,78.03168802018232,96.22759296761404,73.09499809758037,75.85844831279042,72.36925193383885,88.47586499559782,75.80985952982456,69.80378889835472,45.69430680250754,66.58935317747915,59.51428198012956,90.96014789746954,85.59430710452014,90.42453899753964,96.64742716885644,77.15910509073893,87.50879176484702,84.84513684930135,45.35828361091658,48.38028579728175,87.10385094025457,68.77540947206617,64.9319380069486,89.52981289513276],\"showlegend\":true,\"name\":\"Admitted\",\"type\":\"scatter\",\"xaxis\":\"x\",\"x\":[60.18259938620976,79.0327360507101,61.10666453684766,75.02474556738889,76.09878670226257,84.43281996120035,82.30705337399482,69.36458875970939,53.9710521485623,69.07014406283025,70.66150955499435,76.97878372747498,89.6767757507208,77.9240914545704,62.27101367004632,80.1901807509566,61.379289447425,85.40451939411645,52.04540476831827,64.17698887494485,83.90239366249155,94.44336776917852,77.19303492601364,97.77159928000232,62.07306379667647,91.56497449807442,79.94481794066932,99.2725269292572,90.54671411399852,97.64563396007767,74.24869136721598,71.7964620586338,75.3956114656803,40.45755098375164,80.27957401466998,66.74671856944039,64.0393204150601,72.34649422579923,60.45788573918959,58.84095621726802,99.82785779692128,47.26426910848174,50.45815980285988,88.9138964166533,94.83450672430196,67.31925746917527,57.23870631569862,80.36675600171273,68.46852178591112,75.47770200533905,78.63542434898018,94.09433112516793,90.44855097096364,74.49269241843041,89.84580670720979,83.48916274498238,42.2617008099817,99.31500880510394,55.34001756003703,74.77589300092767],\"mode\":\"markers\",\"marker\":{\"symbol\":\"circle\",\"line\":{\"width\":1,\"color\":\"rgba(0, 0, 0, 1.000)\"},\"size\":8,\"color\":\"rgba(0, 154, 250, 1.000)\"}},{\"yaxis\":\"y\",\"y\":[78.0246928153624,43.89499752400101,72.90219802708364,56.3163717815305,38.22527805795094,30.60326323428011,76.03681085115882,46.67857410673128,42.83843832029179,48.85581152764205,44.20952859866288,38.80067033713209,50.25610789244621,64.99568095539578,63.12762376881715,71.16774802184875,52.21388588061123,98.86943574220611,41.57341522824434,75.2377203360134,46.85629026349976,40.61825515970618,45.82270145776001,52.06099194836679,60.39634245837173,49.80453881323059,59.80895099453265,95.59854761387875,47.02051394723416,39.26147251058019,49.59297386723685,66.45008614558913,41.09209807936973,51.88321182073966,43.30717306430063,42.50840943572217,42.71987853716458,78.84478600148043,60.76950525602592,35.57070347228866],\"showlegend\":true,\"name\":\"Not Admitted\",\"type\":\"scatter\",\"xaxis\":\"x\",\"x\":[34.62365962451697,30.28671076822607,35.84740876993872,45.08327747668339,95.86155507093572,75.01365838958247,39.53833914367223,67.94685547711617,67.37202754570876,50.534788289883,34.21206097786789,93.114388797442,61.83020602312595,38.78580379679423,52.10797973193984,40.23689373545111,54.63510555424817,33.91550010906887,74.78925295941542,34.1836400264419,51.54772026906181,82.36875375713919,51.04775177128865,62.22267576120188,34.52451385320009,50.2864961189907,49.58667721632031,32.57720016809309,35.28611281526193,56.25381749711624,30.05882244669796,44.66826172480893,66.56089447242954,49.07256321908844,32.72283304060323,60.45555629271532,82.22666157785568,42.0754545384731,52.34800398794107,55.48216114069585],\"mode\":\"markers\",\"marker\":{\"symbol\":\"circle\",\"line\":{\"width\":1,\"color\":\"rgba(0, 0, 0, 1.000)\"},\"size\":8,\"color\":\"rgba(227, 111, 71, 1.000)\"}}], {\"yaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"Exam 2 Score\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[50.0,100.0],\"domain\":[0.07581474190726165,0.9901574803149606],\"ticktext\":[\"50\",\"100\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"x\"},\"annotations\":[],\"width\":600,\"plot_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"y\":1.0,\"font\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"bordercolor\":\"rgba(0, 0, 0, 1.000)\",\"x\":1.0},\"xaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"Exam 1 Score\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[50.0,100.0],\"domain\":[0.07646908719743364,0.9934383202099737],\"ticktext\":[\"50\",\"100\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"y\"},\"paper_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"height\":400,\"margin\":{\"r\":0,\"l\":0,\"b\":0,\"t\":20}});\n",
       "    </script>\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the data\n",
    "# Subsetting to plot each group separately in order to get separate colors\n",
    "admitted = df[(df[:Admitted].==1),:]\n",
    "notAdmitted = df[(df[:Admitted].==0),:]\n",
    "\n",
    "scatter(admitted[:Exam1Score], admitted[:Exam2Score],\n",
    "    label=\"Admitted\", xlab=\"Exam 1 Score\", ylab=\"Exam 2 Score\")\n",
    "scatter!(notAdmitted[:Exam1Score], notAdmitted[:Exam2Score],\n",
    "    label=\"Not Admitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Converts $z$ into a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition sigmoid(Any"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script src=\"C:\\Users\\JeffM\\.julia\\v0.5\\Plots\\src\\backends\\..\\..\\deps\\plotly-latest.min.js\"></script>    <div id=\"7d4d40b8-7361-4b07-947f-2c7478869f2a\" style=\"width:600px;height:400px;\"></div>\n",
       "    <script>\n",
       "    PLOT = document.getElementById('7d4d40b8-7361-4b07-947f-2c7478869f2a');\n",
       "    Plotly.plot(PLOT, [{\"yaxis\":\"y\",\"y\":[4.5397868702434395e-5,0.00012339457598623172,0.0003353501304664781,0.0009110511944006454,0.0024726231566347743,0.0066928509242848554,0.01798620996209156,0.04742587317756678,0.11920292202211755,0.2689414213699951,0.5,0.7310585786300049,0.8807970779778823,0.9525741268224334,0.9820137900379085,0.9933071490757153,0.9975273768433653,0.9990889488055994,0.9996646498695336,0.9998766054240137,0.9999546021312976],\"showlegend\":true,\"name\":\"y1\",\"type\":\"scatter\",\"xaxis\":\"x\",\"line\":{\"width\":1,\"dash\":\"solid\",\"color\":\"rgba(0, 154, 250, 1.000)\",\"shape\":\"linear\"},\"x\":[-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10],\"mode\":\"lines\"}], {\"yaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[0.25,0.5,0.75],\"domain\":[0.03762029746281716,0.9901574803149606],\"ticktext\":[\"0.25\",\"0.50\",\"0.75\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"x\"},\"annotations\":[],\"width\":600,\"plot_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"y\":1.0,\"font\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"bordercolor\":\"rgba(0, 0, 0, 1.000)\",\"x\":1.0},\"xaxis\":{\"type\":\"-\",\"titlefont\":{\"size\":15,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"title\":\"\",\"tickfont\":{\"size\":11,\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\"},\"ticks\":\"inside\",\"tickmode\":\"array\",\"showgrid\":true,\"tickvals\":[-10.0,-5.0,0.0,5.0,10.0],\"domain\":[0.0658209390492855,0.9934383202099738],\"ticktext\":[\"-10\",\"-5\",\"0\",\"5\",\"10\"],\"tickangle\":0,\"zeroline\":false,\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"tickcolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"y\"},\"paper_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"height\":400,\"margin\":{\"r\":0,\"l\":0,\"b\":0,\"t\":20}});\n",
       "    </script>\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ") in module Main at In[63]:3 overwritten at In[69]:3.\n"
     ]
    }
   ],
   "source": [
    "function sigmoid(z)\n",
    "    # Converts numerical input into a value between 0 and 1\n",
    "    z = 1/(1+exp(-z))\n",
    "    return z\n",
    "end\n",
    "\n",
    "# Plotting values to validate the function is working correctly\n",
    "plot(collect(-10:10), \n",
    "     sigmoid.(collect(-10:10)))  # f. applies the function to the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Hypothesis\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx)$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $g$: Sigmoid function\n",
    "\n",
    "    - $\\theta^T$: Transposed parameters\n",
    "       \n",
    "        - E.x.: $\\theta^T = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logisticHypothesis(Any, Any"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ") in module Main at In[67]:4 overwritten at In[68]:4.\n"
     ]
    }
   ],
   "source": [
    "function logisticHypothesis(theta, X)\n",
    "    # Calculates the hypothesis for X given values of\n",
    "    # theta for logistic regression\n",
    "    X = convert(Array, X)\n",
    "    h = sigmoid.(*(X, theta))\n",
    "    return h\n",
    "end\n",
    "\n",
    "logisticHypothesis(initialTheta, X)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $m$: Number of records\n",
    "\n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition costFunction"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6931471805599452"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Any, Any, Any) in module Main at In[89]:3 overwritten at In[90]:3.\n"
     ]
    }
   ],
   "source": [
    "function costFunction(theta, X, y)\n",
    "    # Computes cost for logistic regression\n",
    "    X = convert(Array, X)\n",
    "    y = convert(Array, y)\n",
    "    m = length(y)\n",
    "    \n",
    "    h = logisticHypothesis(theta, X)\n",
    "    error = sum(-y.*log(h)-(1-y).*log(1-h))\n",
    "    J = (1/m)*error\n",
    "    return J\n",
    "end\n",
    "\n",
    "costFunction(initialTheta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\partial$: Partial derivative\n",
    "    \n",
    "    - $J(\\theta)$: Cost given $\\theta$\n",
    "\n",
    "    - $m$: Number of records\n",
    "    \n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)\n",
    "    \n",
    "We won't actually be using this function to find the optimal values of $\\theta_j$, so this is just illustrating the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition logisticGradient"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  -0.1   \n",
       " -12.0092\n",
       " -11.2628"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Any, Any, Any) in module Main at In[100]:3 overwritten at In[101]:3.\n"
     ]
    }
   ],
   "source": [
    "function logisticGradient(theta, X, y)\n",
    "    # Computes the gradient for logistic regression\n",
    "    X = convert(Array, X)\n",
    "    y = convert(Array, y)\n",
    "    m = length(y)\n",
    "    \n",
    "    h = logisticHypothesis(theta, X)\n",
    "    gradient = (1/m) * (*(transpose(X), (h-y)))\n",
    "    return gradient\n",
    "end\n",
    "\n",
    "logisticGradient(initialTheta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta_j$ for the cost function using the base R optim function.  This is similar to MATLAB's fminunc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the obtained parameters to what Julia's **[insert function here]** function provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on if they are similar]**\n",
    "\n",
    "Calculating the class probability and generating predictions of acceptance using values of $\\theta_j$ obtained from the optimization function\n",
    "\n",
    "The outputs from logistic regression are just the class probability, or $P(y = 1 \\mid x; \\theta)$, so we are predicting the classes (accepted or not) as follows:\n",
    "\n",
    "$Prediction(y \\mid x; \\theta) = \\begin{cases} 1, \\quad\\mbox{ if } P(y = 1 \\mid x; \\theta) > 0.50 \\\\ 0, \\quad\\mbox{ if } P(y = 1 \\mid x; \\theta) \\leq 0.50 \\end{cases} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision boundary over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  **Part 2:** Logistic regression with regularization\n",
    "\n",
    "Predicting if a microchip passes QA after two tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "# Subsetting to plot each group separately in order to get separate colors\n",
    "admitted = df[(df[:Admitted].==1),:]\n",
    "notAdmitted = df[(df[:Admitted].==0),:]\n",
    "\n",
    "scatter(admitted[:Exam1Score], admitted[:Exam2Score],\n",
    "    label=\"Admitted\", xlab=\"Exam 1 Score\", ylab=\"Exam 2 Score\")\n",
    "scatter!(notAdmitted[:Exam1Score], notAdmitted[:Exam2Score],\n",
    "    label=\"Not Admitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping\n",
    "\n",
    "Maps the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.  This allows for a more complex and nonlinear decision boundary.  \n",
    "\n",
    "The feature space prior to feature mapping (3-dimensional vector): \n",
    "\n",
    "$\\hspace{1cm} Feature(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$ \n",
    "\n",
    "The feature space after feature mapping:\n",
    "\n",
    "$\\hspace{1cm} mapFeature(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_1x_2 \\\\ x_2^2 \\\\ x_1^3 \\\\ \\vdots \\\\ x_1x_2^5 \\\\ x_2^6 \\end{bmatrix}$\n",
    "\n",
    "**Note:** I made a few adjustments on the Octave/MATLAB code provided for this assignment in order to maintain the names of the polynomials\n",
    "Octave/MATLAB code:\n",
    "```\n",
    "degree = 6;\n",
    "out = ones(size(X1(:,1)));\n",
    "for i = 1:degree\n",
    "    for j = 0:i\n",
    "        out(:, end+1) = (X1.^(i-j)).*(X2.^j);\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "The only change from the other cost function we used earlier is the addition of the regularization parameter:\n",
    "\n",
    "#### Regularization Parameter\n",
    "\n",
    "$\\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\lambda$: The weight which the parameters are adjusted by.  A lower $\\lambda$ has little effect on the parameters, and a higher $\\lambda$ (e.x. $\\lambda = 1,000$) will adjust the parameters to be close to 0.\n",
    "    - $m$: Number of records\n",
    "    - $j$: The index for the parameter.  E.x. $\\theta_{j=1}$ is the score for Microchip Test #1\n",
    "\n",
    "**Note:** $\\theta_0$ should not be regularized as denoted by the summation in the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\begin{cases} \n",
    "\\hspace{0.25cm} \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} & \\text{for}\\ j = 0 \\\\\n",
    "\\Big(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\\Big) + \\frac{\\lambda}{m}\\theta_j & \\text{for}\\ j \\geq 1\n",
    "\\end{cases}$\n",
    "\n",
    "This is also the same as the last gradient with the exception of the regularization parameter\n",
    "\n",
    "#### Regularization Parameter\n",
    "\n",
    "$\\frac{\\lambda}{m}\\theta_j \\hspace{0.5cm}$for $j \\geq 1$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\lambda$: The weight which the parameters are adjusted by.  A lower $\\lambda$ has little effect on the parameters, and a higher $\\lambda$ (e.x. $\\lambda = 1,000$) will adjust the parameters to be close to 0.\n",
    "    - $m$: Number of records\n",
    "    - $j$: The index for the parameter.  E.x. $\\theta_{j=1}$ is the score for Microchip Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta$.  This chunk will take longer to run since we're dealing with a much higher dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking against Julia's **[Insert function here]** logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on the difference]**\n",
    "\n",
    "Lastly, comparing the accuracy between the two models.  Classification accuracy is just the percentage of records correctly classified (precision, recall, f-1 score, etc. offer more nuanced information on performance), so we will have to calculate the class probabilities and assign predictions like we did for part one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Comment on the difference]**\n",
    "\n",
    "Plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
