{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Python: Logistic Regression\n",
    "\n",
    "## *Part One*: Logistic regression without regularization\n",
    "\n",
    "Beginning with package imports, data loading, and initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.discrete.discrete_model as sm  # For comparing answers\n",
    "from scipy import optimize  # Discovering optimal parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/JeffM/Documents/Projects/Machine Learning/machine-learning-ex2/ex2/'\n",
    "\n",
    "df = pd.read_csv(path+'ex2data1.txt', header=None, names=['Exam1Score', 'Exam2Score', 'Admitted'])\n",
    "\n",
    "# Inserting additional column for the intercept\n",
    "df['x0'] = 1\n",
    "\n",
    "X = df[['x0', 'Exam1Score', 'Exam2Score']]\n",
    "y = df['Admitted']\n",
    "\n",
    "# An array of 0s for starting values of theta to be used in many functions\n",
    "initialTheta = np.zeros(3)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "sns.lmplot(x='Exam1Score', y='Exam2Score', hue='Admitted',\n",
    "            data=df, fit_reg=False, markers=[\"x\", \"o\"])\n",
    "plt.xlabel('Exam 1 Score')\n",
    "plt.ylabel('Exam 2 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Converts $z$ into a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1/(1+np.exp(-z)))\n",
    "\n",
    "# Plotting values to validate the function is working correctly\n",
    "plt.plot(np.arange(-10,10),\n",
    "         sigmoid(np.arange(-10,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Hypothesis\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx)$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $g$: Sigmoid function\n",
    "\n",
    "    - $\\theta^T$: Transposed parameters\n",
    "       \n",
    "        - E.x.: $\\theta^T = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_hypothesis(theta, x):\n",
    "    \"\"\"Calculates the hypothesis for X given values of\n",
    "    theta for logistic regression\"\"\"\n",
    "    return(sigmoid(x.dot(theta)))\n",
    "\n",
    "logistic_hypothesis(theta=initialTheta, x=X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $m$: Number of records\n",
    "\n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(theta, X, y):\n",
    "    \"\"\"Computes cost for logistic regression\"\"\"\n",
    "    m = y.size\n",
    "    h = logistic_hypothesis(theta, X)\n",
    "    error = np.sum(-y*np.log(h)-(1-y)*np.log(1-h))\n",
    "    J = (1/m)*error\n",
    "    return(J)\n",
    "\n",
    "cost_function(theta=initialTheta, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "- Notation:\n",
    "\n",
    "    - $\\partial$: Partial derivative\n",
    "    \n",
    "    - $J(\\theta)$: Cost given $\\theta$\n",
    "\n",
    "    - $m$: Number of records\n",
    "    \n",
    "    - $h_\\theta$: Logistic hypothesis $(h)$ given specific values of $\\theta$ for parameters\n",
    "    \n",
    "    - $i$: Index of the record (e.x. if $i = 46$, then 46th row)\n",
    "    \n",
    "We won't actually be using this function to find the optimal values of $\\theta_j$, so this is just illustrating the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    \"\"\"Computes the gradient for logistic regression\"\"\"\n",
    "    m = y.size\n",
    "    h = logistic_hypothesis(theta, X)\n",
    "    return (1/m)*(np.dot(X.values.T, (h.subtract(y, axis=0))))\n",
    "\n",
    "gradient(theta=initialTheta, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal values of $\\theta_j$ for the cost function using scipy's fmin function from their optimize suite.  This is similar to MATLAB's fminunc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of theta that minimize the cost function\n",
    "optimalTheta = optimize.fmin(func=cost_function,  # Function to minimize\n",
    "                             x0=initialTheta,  # Initial guess\n",
    "                             args=(X, y))  # Additional Arguments\n",
    "\n",
    "# Pretty printing the obtained values for theta\n",
    "print('\\nOptimal Thetas:')\n",
    "for theta in enumerate(optimalTheta):\n",
    "    print('Theta', theta[0], ':', theta[1])\n",
    "    \n",
    "print('\\nCost:', cost_function(optimalTheta, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the obtained parameters to what statsmodels provides\n",
    "\n",
    "Using statsmodels instead of scikit-learn due to scikit-learn automatically regularizing the parameters.  Part one focuses on unregularized logistic regression, and part two focuses on regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model = sm.Logit(y, X)\n",
    "\n",
    "# Outputting model parameters\n",
    "model = model.fit().params\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are very close!\n",
    "\n",
    "Calculating the class probability and generating predictions of acceptance using values of $\\theta_j$ obtained from the optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the class probability with the obtained thetas\n",
    "df['ClassProbability'] = logistic_hypothesis(optimalTheta, X)\n",
    "\n",
    "# Assigning those with a class probability above 0.5 as admitted\n",
    "df['Prediction'] = np.where(df['ClassProbability'] > 0.5, 1, 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision boundary over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "sns.lmplot(x='Exam1Score', y='Exam2Score', hue='Admitted',\n",
    "            data=df, fit_reg=False, markers=[\"x\", \"o\"], legend=False)\n",
    "\n",
    "# Calculating and plotting the decision boundary\n",
    "decisionX = np.array([X['Exam1Score'].min(), X['Exam1Score'].max()])\n",
    "decisionY = (-1/optimalTheta[2])*(optimalTheta[0] + optimalTheta[1]*decisionX)\n",
    "plt.plot(decisionX, decisionY, label='Decision Boundary',\n",
    "         color='black', alpha=0.8, linestyle='--')\n",
    "\n",
    "# Adjusting the legend location\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.6), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Part Two:** Logistic regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+'ex2data2.txt', header=None, names=['Test1', 'Test2', 'Accepted'])\n",
    "\n",
    "# Inserting additional column for the intercept\n",
    "df['x0'] = 1\n",
    "\n",
    "X = df[['x0', 'Test1', 'Test2']]\n",
    "y = df['Accepted']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='Test1', y='Test2', hue='Accepted',\n",
    "            data=df, fit_reg=False, markers=['x', 'o'])\n",
    "plt.xlabel('Microchip Test 1')\n",
    "plt.ylabel('Microchip Test 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "**Note:** $\\theta_0$ should not be regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(theta, X, y, C):\n",
    "    \"\"\"Computes cost for a regularized logistic regression\"\"\"\n",
    "    m = y.size\n",
    "    h = logistic_hypothesis(theta, X)\n",
    "    error = np.sum(-y*np.log(h)-(1-y)*np.log(1-h))\n",
    "    \n",
    "    # Calculating the regularization penalty\n",
    "    # Avoiding the regularization penalty for the first theta\n",
    "    regularizedTheta = [x**2 for x in theta[1:]]\n",
    "    regularization = (C/(2*m))*np.sum(regularizedTheta)\n",
    "    \n",
    "    J = (1/m)*error + regularization\n",
    "    return(J)    \n",
    "\n",
    "# Testing how cost differs with regularization\n",
    "# Using part 1 optimal thetas to test if the function is working\n",
    "# If using initialTheta, there would be no impact since the values are 0\n",
    "print(regularized_cost(optimalTheta, X, y, 0))\n",
    "print(regularized_cost(optimalTheta, X, y, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Gradient\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\Big(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\\Big) + \\frac{\\lambda}{m}\\theta_j \\hspace{0.5cm}$for $j \\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(optimalTheta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model = sm.Logit(y, X)\n",
    "\n",
    "# Outputting model parameters\n",
    "model = model.fit().params\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', C=1.0)\n",
    "model.fit(X, y)\n",
    "model.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
