{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T18:13:02.283307Z",
     "start_time": "2017-11-07T18:12:55.662350Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()  # Run if first time using nltk on this machine\n",
    "\n",
    "# Documents to be analyzed\n",
    "document1 = 'Sushi is some of my favorite food, but I am not a fan of complicated rolls'\n",
    "document2 = 'I loved the show, it was so amazing!'\n",
    "document3 = 'I hated everything about that book.  The writing was awful, and I hated the characters'\n",
    "\n",
    "# Collecting the documents into a corpus\n",
    "corpus = [document1, document2, document3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis w/ NLTK\n",
    "\n",
    "**Note:** No text preprocessing needs to be done with this package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T18:13:02.375884Z",
     "start_time": "2017-11-07T18:13:02.285311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sushi is some of my favorite food, but I am not a fan of complicated rolls\n",
      "neg 0.149\n",
      "neu 0.73\n",
      "pos 0.122\n",
      "compound -0.1136\n",
      "\n",
      "I loved the show, it was so amazing!\n",
      "neg 0.0\n",
      "neu 0.356\n",
      "pos 0.644\n",
      "compound 0.8767\n",
      "\n",
      "I hated everything about that book.  The writing was awful, and I hated the characters\n",
      "neg 0.533\n",
      "neu 0.467\n",
      "pos 0.0\n",
      "compound -0.9081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jemacalu\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in corpus:\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in ss:\n",
    "        print(k, ss[k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis w/ dictionaries\n",
    "\n",
    "$$Sentiment = \\frac{(Positive\\ Words - Negative\\ Words)}{Total\\ Words}$$\n",
    "\n",
    "Using the [Sentiment Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon) from Professor Bing Liu of the University of Illinois at Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T18:13:02.421908Z",
     "start_time": "2017-11-07T18:13:02.378893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sushi', 'favorite', 'food', 'fan', 'complicated', 'rolls']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the positive and negative dictionaries\n",
    "positive_dict = set(line.strip().lower() for line in open('./data/positive-words.txt'))\n",
    "negative_dict = set(line.strip().lower() for line in open('./data/negative-words.txt'))\n",
    "\n",
    "\n",
    "def preprocess(document):\n",
    "    \"\"\"\n",
    "    Normalizes, tokenizes, and removes stop words from documents\n",
    "    \"\"\"\n",
    "    # Converting all words to lower case\n",
    "    normalized = document.lower()\n",
    "    \n",
    "    # Tokenizing sentence & removing punctuation\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(normalized)\n",
    "    \n",
    "    # Filtering out stop words\n",
    "    filtered_words = [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "    # Words in dictionary are not stemmed, so this doesn't need to be done\n",
    "#     stemmer = nltk.stem.PorterStemmer()\n",
    "#     stemmed = [stemmer.stem(word) for word in filtered_words]\n",
    "#     return stemmed\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "preprocess(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T18:13:02.458954Z",
     "start_time": "2017-11-07T18:13:02.425416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sushi is some of my favorite food, but I am not a fan of complicated rolls\n",
      "Negative words: 1\n",
      "Positive words: 1\n",
      "Total words: 6\n",
      "Sentiment: 0.0\n",
      "\n",
      "I loved the show, it was so amazing!\n",
      "Negative words: 0\n",
      "Positive words: 2\n",
      "Total words: 3\n",
      "Sentiment: 0.6666666666666666\n",
      "\n",
      "I hated everything about that book.  The writing was awful, and I hated the characters\n",
      "Negative words: 3\n",
      "Positive words: 0\n",
      "Total words: 7\n",
      "Sentiment: -0.42857142857142855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis(document):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis using the \n",
    "    \"\"\"\n",
    "    # Normalizing, tokenizing, and removing stop words\n",
    "    processed_document = preprocess(document)\n",
    "    \n",
    "    # Counting the number of positive/negative words in the dictionary\n",
    "    positive_words = sum([word in positive_dict for word in processed_document])\n",
    "    negative_words = sum([word in negative_dict for word in processed_document])\n",
    "    total_words = len(processed_document)\n",
    "    \n",
    "    # Calculating the sentiment\n",
    "    sentiment = (positive_words - negative_words) / total_words\n",
    "    \n",
    "    # Formatting the output\n",
    "    return sentiment, positive_words, negative_words, total_words\n",
    "\n",
    "\n",
    "# Iterating through the corpus to retrive sentiment by document\n",
    "for document in corpus:\n",
    "    sentiment = sentiment_analysis(document)\n",
    "    print(document)\n",
    "    print('Negative words: {}'.format(sentiment[2]))\n",
    "    print('Positive words: {}'.format(sentiment[1]))\n",
    "    print('Total words: {}'.format(sentiment[3]))\n",
    "    print('Sentiment: {}\\n'.format(sentiment[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
